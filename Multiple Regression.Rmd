---
title: "Regression Project"
author: "Thomas Townsley, John Carr, Matthew Lewis"
date: "December 7th, 2018"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(dplyr)
library(readr)
library(car)
library(broom)
library(corrplot)

```
## Abstract
Our goal with this project was to build a model that would predict the price of a home in the Washington D.C. area. We used a dataset that included 2,000 observations of other homes in the area with pertinent data to power the model. Our final model had an R2 of 0.75, which we beleive to be strong. When we run the predictor three out of the five lines we removed from the dataset are predicted acurately. Our conclusion is that our model would allow us to achieve the business need we set out to solve.

```{r 1.1}

```
## Introduction

With our model, if we had pertinent data on a home in D.C., but not the price, we will be able to predict what the price should be. Also, this model would be useful in determining what the price of a home in DC should cost even if there is a price already associated with it. For example, if one were to be in the market for such a home, this model would help the buyer or seller understand what would be a reasonable final price. This would inform the amount a buyer could/should offer, or the price a seller could justifiably list the home at. Data such as land area of the home, number of rooms, year built, etc. will power the predictive algorithym. As we began the discovery of this project, we anticipated that variables such as land area and number of rooms would be the most significant.

```{r 2.1}

```
## Overview of the dataset

Our dataset is titled D.C. Residential Properties and was acquired on kaggle.com. This dataset shows real property information, including most recent sales prices as of July 2018, for properties located in Washington, D.C. There are a number of variables that we beleive would be useful to us in achieving our goal, with "Condition" being the main conditional variable we use. We eliminated some variables that we decided would not be relevant or useful immediately. 

One way we manipulated the original dataset is by transforming the SALEDATE variable into separate variables for SALEYEAR, SALEMONTH and SALEDAY. We did this because we found little significance to SALEDATE as each was unique and spreadout, and thought there might be more significance in using individual parts of the dates at higher levels. We kept the original SALEDATE column in the dataset, as well.

Description of variables:
BATHRM (numeric): Number of full bathrooms
HF_BATHRM (numeric): Number of half bathrooms (no bathtub or shower)
ROOMS (numeric): Total number of rooms
BEDRM (numeric): Number of bedrooms
AYB (numeric): The earliest time the main portion of the building was built
YR_RMDL (numeric): Year structure was remodeled
STORIES (numeric): Number of stories in primary dwelling
SALEDATE (date): Date of most recent sale
SALEMONTH (numeric): Column we created for the calendar month of the most recent sale
SALEDAY (numeric): Column we created for the calendar day of the most recent sale
SALEYEAR (numeric): Column we created for the calendar year of the most recent sale
GBA (numeric): Gross building area in square feet
CNDTN (factor): Condition
LANDAREA (numeric): Land area of property in square feet
PRICE (numeric): Price of most recent sale

```{r 3.1}

```

```{r 4.1}

# 4. Exploratory analysis

houses <- read.csv("DC_properties_clean.csv")

excl_rows <- houses[sample(nrow(houses), 5), ] # Exclude Random Rows
houses <- setdiff(houses, excl_rows)

houses$CNDTN <- factor(houses$CNDTN, order = TRUE, levels = c("Fair", "Average", "Good", "Very Good", "Excellent"))

fit <- lm(data = houses, PRICE ~ BATHRM + HF_BATHRM  + ROOMS + BEDRM + AYB + YR_RMDL + STORIES + SALEMONTH + SALEDAY + SALEYEAR + GBA + CNDTN + LANDAREA)



#Histograms to examine distribution of each variable

ggplot(houses, aes(x = BATHRM))+
  geom_histogram(color="black", fill="light gray", binwidth = 1)+
  labs(x="Bathrooms", title="Histogram of Bathrooms")

ggplot(houses, aes(x = HF_BATHRM))+
  geom_histogram(color="black", fill="light gray", binwidth = 1)+
  labs(x="Half Bathrooms", title="Histogram of Half Bathrooms")

ggplot(houses, aes(x = ROOMS))+
  geom_histogram(color="black", fill="light gray", binwidth = 1)+
  labs(x="Rooms", title="Histogram of Rooms")

ggplot(houses, aes(x = BEDRM))+
  geom_histogram(color="black", fill="light gray", binwidth = 1)+
  labs(x="Bed Rooms", title="Histogram of Bed Rooms")

ggplot(houses, aes(x = AYB))+
  geom_histogram(color="black", fill="light gray", binwidth = 10)+
  labs(x="Year Built", title="Histogram of Year Built")

ggplot(houses, aes(x = YR_RMDL))+
  geom_histogram(color="black", fill="light gray", binwidth = 10)+
  labs(x="Year Remodelled", title="Histogram of Year Remodelled")

ggplot(houses, aes(x = STORIES))+
  geom_histogram(color="black", fill="light gray", binwidth = 1)+
  labs(x="Stories", title="Histogram of Stories")

ggplot(houses, aes(x = SALEYEAR))+
  geom_histogram(color="black", fill="light gray", binwidth = 5)+
  labs(x="Year Sold", title="Histogram of Year Sold")

ggplot(houses, aes(x = SALEMONTH))+
  geom_histogram(color="black", fill="light gray", binwidth = 1)+
  labs(x="Month Sold", title="Histogram of Month Sold")

ggplot(houses, aes(x = SALEDAY))+
  geom_histogram(color="black", fill="light gray", binwidth = 4)+
  labs(x="Day Sold", title="Histogram of Day Sold")

ggplot(houses, aes(x = GBA))+
  geom_histogram(color="black", fill="light gray", binwidth = 100)+
  labs(x="Square Footage", title="Histogram of Square Footage")

ggplot(houses, aes(x = LANDAREA))+
  geom_histogram(color="black", fill="light gray", binwidth = 100)+
  labs(x="Land Area", title="Histogram of Land Area")

ggplot(houses, aes(x = CNDTN))+
  geom_bar(color="black", fill="light gray")+
  labs(x="Land Area", title="Bar Plot of Land Area")

# Boxplots to identify outliers

boxplot(houses$BATHRM, main="Bathrooms", sub=paste("Outlier rows: ", boxplot.stats(houses$BATHRM)$out))

boxplot(houses$HF_BATHRM, main="Half Bathrooms", sub=paste("Outlier rows: ", boxplot.stats(houses$HF_BATHRM)$out))

boxplot(houses$ROOMS, main="Rooms", sub=paste("Outlier rows: ", boxplot.stats(houses$ROOMS)$out))

boxplot(houses$BEDRM, main="Bedrooms", sub=paste("Outlier rows: ", boxplot.stats(houses$BEDRM)$out))

boxplot(houses$AYB, main="Year Built", sub=paste("Outlier rows: ", boxplot.stats(houses$AYB)$out))

boxplot(houses$YR_RMDL, main="Year Remodeled", sub=paste("Outlier rows: ", boxplot.stats(houses$YR_RMDL)$out))

boxplot(houses$STORIES, main="Stories", sub=paste("Outlier rows: ", boxplot.stats(houses$STORIES)$out))

boxplot(houses$SALEYEAR, main="Year of Most Recent Sale", sub=paste("Outlier rows: ", boxplot.stats(houses$SALEYEAR)$out))

boxplot(houses$GBA, main="Gross Building Area", sub=paste("Outlier rows: ", boxplot.stats(houses$GBA)$out))

boxplot(houses$CNDTN, main="Condition", sub=paste("Outlier rows: ", boxplot.stats(houses$CNDTN)$out))

boxplot(houses$LANDAREA, main="Land Area", sub=paste("Outlier rows: ", boxplot.stats(houses$LANDAREA)$out))

boxplot(houses$PRICE, main="Price", sub=paste("Outlier rows: ", boxplot.stats(houses$PRICE)$out))

# Boxplots to identify outliers

boxa <- boxplot(houses$BATHRM, main="Bathrooms", sub=paste("Outlier rows: ", boxplot.stats(houses$BATHRM)$out))

boxb <- boxplot(houses$HF_BATHRM, main="Half Bathrooms", sub=paste("Outlier rows: ", boxplot.stats(houses$HF_BATHRM)$out))

boxc <- boxplot(houses$ROOMS, main="Rooms", sub=paste("Outlier rows: ", boxplot.stats(houses$ROOMS)$out))


# scatterplots
# price by room
room_price <- ggplot(houses, aes(x = ROOMS, y = PRICE))
room_price + geom_point()

# price by year
year_price <- ggplot(houses, aes(x = SALEYEAR, y = PRICE))
year_price + geom_point()

# price by month
month_price <- ggplot(houses, aes(x = SALEMONTH, y = PRICE))
month_price + geom_point()

# price by day of month
day_price <- ggplot(houses, aes(x = SALEDAY, y = PRICE))
day_price + geom_point()


# Use corrplot to display a correlation matrix of that variables
library(corrplot)
corr_matrix <- cor(data.frame(houses$HF_BATHRM, houses$BATHRM, houses$ROOMS, houses$BEDRM,houses$AYB, houses$YR_RMDL, houses$PRICE, houses$SALEDAY, houses$SALEMONTH, houses$SALEYEAR, houses$STORIES, houses$LANDAREA, houses$GBA))
corrplot(corr_matrix, type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45)


# capture model summary as an object
modelSummary <- summary(fit)

# model coefficients
modelCoeffs <- modelSummary$coefficients



```
## Identification and Evaluation of Suitable Model

```{r 5.1}

# 5. Identification and evaluation of suitable model

fit <- lm(data = houses, PRICE ~ BATHRM + HF_BATHRM  + ROOMS + BEDRM + AYB + YR_RMDL + STORIES + SALEMONTH + SALEDAY + SALEYEAR + GBA + CNDTN + LANDAREA)

# Review p-values, check sign of estimates

summary(fit)

```

```{r 5.2}

# Review ANOVA and compare F statistic

anova(fit)

```

```{r 5.3}

# Residual plot and look for non-constant variance

fit.df <- augment(fit)
ggplot(fit.df, aes(x = .fitted, y = .resid))+
  geom_point()+
  geom_hline(yintercept = 0, linetype = 2)+
  labs( x = "Fitted Values", y = "Residuals")

```

```{r 5.4}
# Shapiro-Wilk test for normality

shapiro.test(fit$residuals)

```

```{r 5.5}
# ncvTest for constant variance

ncvTest(fit)

```

```{r 5.6}

# Variance Inflation Factor (used to identify multicllinearity >= 10)

vif(fit)

```

```{r 5.7}
# Review R2 value
# From summary above: adjusted R^2 = .6984

# Drop non-significant predictor variables (Droped AYB: +.0002, Rooms: No change, other lower)
fit <- lm(data = houses, PRICE ~ BATHRM + HF_BATHRM  + BEDRM + YR_RMDL + STORIES + SALEMONTH + SALEDAY + SALEYEAR + GBA + CNDTN + LANDAREA)
summary(fit)

# If multicollinearity is present, drop one correlated variable (None present)

# Transform the outcome variable if the residual plot looks suspicious
powerTransform(houses$PRICE) #.33 -> sqrt

```

```{r 5.8, echo=FALSE}

# log lowers R2: .6585
fit <- lm(data = houses, log(PRICE) ~ BATHRM + HF_BATHRM  + BEDRM + YR_RMDL + STORIES + SALEMONTH + SALEDAY + SALEYEAR + GBA + CNDTN + LANDAREA)
summary(fit)
# sqrt increase R2: .7471, drop YR_RMDL + .0001
fit <- lm(data = houses, sqrt(PRICE) ~ BATHRM + HF_BATHRM  + BEDRM  + STORIES + SALEMONTH + SALEDAY + SALEYEAR + GBA + CNDTN + LANDAREA)
summary(fit)
# inverse drastically decrease
fit <- lm(data = houses, I(1/PRICE) ~ BATHRM + HF_BATHRM  + BEDRM  + STORIES + SALEMONTH + SALEDAY + SALEYEAR + GBA + CNDTN + LANDAREA)
summary(fit)

#Variable Transformations
powerTransform(houses$BATHRM+1) # -.011
powerTransform(houses$HF_BATHRM+1) # .776
powerTransform(houses$BEDRM+1) # -.130
powerTransform(houses$STORIES+1) # -2.084
powerTransform(houses$SALEMONTH) # .809
powerTransform(houses$SALEYEAR-1990) # 1.48 - didn't help
powerTransform(houses$GBA) # -.08 -> LOG
powerTransform(houses$LANDAREA) # .35 -> SQRT

fit_final <- lm(data = houses, sqrt(PRICE) ~  BATHRM  + HF_BATHRM  + BEDRM + log(BEDRM+1) + STORIES  + SALEMONTH + SALEDAY + SALEYEAR  + GBA + log(GBA) + CNDTN + LANDAREA )
summary(fit_final) # R^2 increased to .7558

fit.df <- augment(fit)
ggplot(fit.df, aes(x = .fitted, y = .resid))+
  geom_point()+
  geom_hline(yintercept = 0, linetype = 2)+
  labs( x = "Fitted Values", y = "Residuals")

```

```{r 5.9}

shapiro.test(fit$residuals)
ncvTest(fit)
vif(fit)

#using AIC & BIC for model comparison
AIC(fit) # AIC
BIC(fit) # BIC

AIC(fit_final) # AIC 
BIC(fit_final) # BIC 

# The AIC and BIC for fit_final are both lower, so that is the better model

```
## Application of the Model

```{r 6.1}

# 6. Application of the model 

# Test on excluded rows
predict(fit, newdata = excl_rows, interval = "predict")

```
## Limitations and Assumptions

One limitation to the original iteration of our dataset was the SALEDATE. As an independent variable there were too many for them to be significant or to evaluate each individually. Also, the format of SALEDATE did not work well, especially due to the "0:00" after each date, presumably a timestamp. As mentioned in section 2, we split this variable to create new variables for each part of the date. Doing so resulted in new significant variables that improved our model.

In the future we would potentially include geographic data. In the original dataset there was longitude and latitude data, which was excluded from the dataset. Retrospectively, not being able to identify neighborhoods where homes are could be a limitation and including that data could make the model more accurate and useful.

One assumption that we make with this dataset, and thus our model, is the "Condition" variable. I would be interested in knowing who determined the condition of each home, when and by what means of evaluation. Is there a standard for such an assessment? This seems like something that could be subjective. Also we notice that there is no "poor" condition - the lowest level is "Fair", which could perpetuate subjectivity.

```{r 7.1}

```



